%---------------------------------------------------------------------
%
%                          Capítulo 4
%
%---------------------------------------------------------------------
\chapter{Entrenamiento}
\label{cap4}

\begin{resumen}
Utilizando los conjuntos de imágenes reales y sintéticas obtenidos, se procede al entrenamiento necesario para dotar a la aplicación de las capacidad de reconocimiento de imágenes. Para elegir la mejor relación proporcional entre reales y sintéticas se realizan una serie de ensayos, cuyos resultados se comparan y estudian para seleccionar la mejor relación entre facilidad de obtención del \textit{dataset} y rendimiento de la red.

%En este capítulo se explican las características de la red neuronal, necesaria para llevar a cabo la identificación de objetos. Tensorflow~Lite ha sido el framework seleccionado y a partir del cual se ha desarrollado el script de entrenamiento. Para elegir la mejor relación entre imágenes sintéticas y reales que utilizar para la aplicación, se ha realizado una investigación probando con diversas proporciones y comparando los resultados.

\end{resumen}

%-------------------------------------------------------------------
\section{Entrenamiento}
%-------------------------------------------------------------------
\label{cap4:sec:entrenamiento}


La fase de entrenamiento del proyecto está basada en las recomendaciones para generar modelos de TensorFlow~Lite, ya que el modelo que se va a utilizar para la identificación es el de este formato.
Para llevar a cabo el entrenamiento de la red neuronal se ha realizado un \textit{script} que se encarga de la lógica de esto. Este \textit{script} está basado en uno de los ejemplos de Tensorflow~Lite y puede usarse desde Colaboratory\footnote{\url{https://colab.research.google.com/notebooks/intro.ipynb?utm_source=scs-index}}, plataforma de Google que permite ejecutar y programar en Python desde el navegador. El ejemplo elegido como referencia consiste en identificación de flores\footnote{\url{https://colab.research.google.com/drive/1sqBewUnvdATOO-yblj55EBFb2sM24XHR?hl=es-419}}, se ha seleccionado este debido a su similitud con el proyecto respecto a la identificación del objeto principal en un imagen. 

%Este utiliza la librería Task de TensorFlow~Lite, la cual contiene herramientas para que los desarrolladores creen experiencias de aprendizaje automático con Tensorflow~Lite, tales como un clasificador de imágenes, detector de objetos o clasificador de lenguaje natural, entre otras. 
%También la biblioteca Model~Maker, la cual simplifica el proceso de adaptación y conversión de un modelo de red neuronal de TensorFlow para aplicaciones de aprendizaje automático. Y por último, MobileNetV2\citep*{mobilenev22018}, que utiliza convolución separable en profundidad haciendo la red más eficiente.

Para generar el modelo usando las imágenes obtenidas se utiliza la biblioteca Model Maker de TensorFlow Lite, que permite llevarlo a cabo de manera sencilla mediante unas pocas líneas de código. El entrenamiento para este proyecto se lleva a cabo desde la computadora, evitando tener que cargar todo el conjunto de imágenes en Colaboratory, para ello se tiene un \textit{script}, similar al del ejemplo, que realiza la carga de las imágenes, el entrenamiento, la creación del modelo y las pruebas de rendimiento.

En el entrenamiento de las redes neuronales, generalmente se utiliza la mayor parte de los datos etiquetados para el entrenamiento en sí y los restantes de prueba, para medir la precisión de la red. Puesto que el código tomado de ejemplo está planteado para datasets formados únicamente por imágenes reales, la carga de estas se hace en conjunto y son separadas posteriormente en un 90\% para entrenamiento y el 10\% restante para \textit{test}.

En cambio, en este proyecto, puesto que el entrenamiento se va a realizar a partir de imágenes generadas y reales mezcladas, y posteriormente se va a utilizar sobre objetos reales, para comprobar la viabilidad del modelo es necesario que las imágenes de \textit{test} sean capturas reales solamente. Por lo tanto, en este caso, es necesario cargar las imágenes de entrenamiento y test ya separadas. Para dicha distribución se ha mantenido aproximadamente la proporción de 90\% y 10\% respectivamente. 
Con esto, la estructura de carpetas frente al entrenamiento queda como se muestra en la figura~\ref{fig:CarpetasSepar}. Son necesarias dos carpetas principales, una en la que se guardan los datos de entrenamiento y en la otra los de test. Al tratarse de una red con aprendizaje supervisado, los datos deben ir correctamente etiquetados. Esto se traduce en que ambas carpetas contienen tres subcarpetas, una para cada material identificable (plástico, vidrio y metal), en las que se guardan las imágenes correspondientes. El nombre de estas subcarpetas será lo que se tome como etiquetas para la red.

\figura{Estructuracarpetas3.pdf}{width=1\textwidth}{fig:CarpetasSepar}{Organización de las carpetas con las imágenes separadas.}

El entrenamiento se divide en varios episodios. Estos corresponden al número de veces que el algoritmo recorre todos los datos. En cada episodio los datos se dividen en lotes, y en cada lote se recoge una pequeña parte de los datos. Cada lote se recorre en una iteración, por lo que cada episodio tiene las iteraciones necesarias para recorrer al completo el conjunto de datos. Esto significa que el número de iteraciones de cada episodio corresponde al resultado de dividir la cantidad de datos entre el tamaño de lote~\citep*{warden2019tinyml}. Por ejemplo, si se tienen 2400 imágenes y el tamaño de lote es 32, en cada episodio habría 75 iteraciones.
Para estos valores se han mantenido los de por defecto de TensorFlow ya que se ha considerado que eran adecuados; siendo el número de episodios 5 y el tamaño de lote 32.


Con todo lo mencionado se genera el modelo entrenado con extensión \textit{.tflite}, además de un documento de texto plano con las etiquetas de los distintos materiales. Estos dos archivos son lo que se han de trasladar a la aplicación móvil para ser utilizados.

%-------------------------------------------------------------------
\section{Resultados}
%-------------------------------------------------------------------
\label{cap4:sec:resultados}

%\todo{On the other hand, it is known that classifiers trained only with virtual images may require domain adaptation to work on real images [42, 44, 37, 25, 45]; however, it has been shown that this is just because virtual and real world cameras are different sensors, i.e. domain adaptation is also often required when training images and testing images come from different real-world camera sensors [42, 41].\cite{synthia2016}}
%\todo{the use of the combined data significantly boosts the performance obtained when using the real-world data alone. \cite{synthia2016}}
\todo{¿meter la pérdida, \textit{loss}?}
Como último punto del entrenamiento, queda investigar con qué porcentaje de imágenes sintéticas se obtiene el mejor resultado en precisión que además el correcto funcionamiento de la aplicación.
 
Esta investigación se ha llevado a cabo con el \textit{dataset} obtenido para el prototipado (explicado en la sección~\ref{cap3:sec:dataset}) de la aplicación. Esto supone que sólo uno de los tres materiales (metal) cuenta con imágenes sintéticas y reales mezcladas. En cambio, los dos materiales restantes (vidrio y plástico) están formados por completo por imágenes reales.

Para la comparación se ha comenzando con sólo imágenes reales, posteriormente se han ido aumentando paulatinamente el número de imágenes generadas en un 10\% hasta contar finalmente con un \textit{dataset} de sólo imágenes sintéticas.
La figura~\ref{fig:VarRN} corresponde al crecimiento de la precisión de la red durante el entrenamiento. La precisión es el porcentaje de aciertos de la red respecto a los resultados reales. Se encuentra representado para los distintas combinaciones de imágenes generadas y reales. En todos los casos crece de manera similar sin haber grandes diferencias según la proporción de imágenes sintéticas.
 
En la figura~\ref{fig:TestRN}, por el contrario, se observa cómo varía la precisión al probarse el modelo con los datos de \textit{test}, aquí se sacan las primeras conclusiones sobre la red. Se observa que la mayoría de las opciones se mantienen casi todo el proceso en una precisión mayor del 90\%. 
Los casos en los que se puede contemplar una caída de la precisión, son aquellos en los que más del 80\% de las imágenes son sintéticas. 

La figura~\ref{fig:PrecisionRN} muestra el valor final de la precisión en los distintos casos. A partir de esta última figura pueden sacarse las conclusiones de con qué porcentaje se obtendrían los mejores resultados. 

Los dos \textit{datasets} que cuentan con la mayor cantidad de imágenes sintéticas serían los menos recomendados para utilizar debido a su baja precisión. En cambio, entre el 0\% y el 70\% de imágenes generadas, se observa que la precisión siempre está por encima del 90\%, aunque nunca  llega a superar el 95\%. 

Entrenando la red únicamente con los materiales de vidrio y plástico, cuyas imágenes son todas reales, se obtiene un 96\% de precisión.
Al comparar esto y el resultado del entrenamiento de la red con solamente imágenes descargadas, cuya precisión es del 93\%. Se observa que las imágenes sintéticas, siempre que no sean más del 70\% del total del \textit{dataset}, mantienen la precisión en valores prácticamente iguales, o incluso aumentándola levemente en algunos casos. Con estos datos se puede afirmar que la mezcla de imágenes sintéticas y reales para el entrenamiento tiene resultados sumamente satisfactorios.

De cara a elegir cuál de los modelos entrenados utilizar en la aplicación para dispositivos móviles, se tiene en cuenta el rendimiento de la red y la facilidad de obtención del \textit{dataset}. Esto se traduce en que se busca un modelo con un alto nivel de precisión pero que también cuente con un gran número de imágenes generadas, ya que estas, con lo desarrollado en el proyecto, se pueden conseguir de manera más fácil que las reales.
\todo{ampliar esto en el capítulo 3}

Finalmente, se considera como mejor opción el modelo entrenado con un 70\% de imágenes generadas en el material de metal. Esta opción es de las que mayor precisión tienen y a la vez permite tener un \textit{dataset} que funciona adecuadamente formado principalmente por imágenes sintéticas, lo que facilita su obtención.

\figura{Epochos.pdf}{width=1\textwidth}{fig:VarRN}{Variación de la precisión de la red neuronal a lo largo del entrenamiento de esta.}
\figura{Test.pdf}{width=1\textwidth}{fig:TestRN}{Variación de la precisión de la red neuronal durante las pruebas de esta.}
\figura{Final.pdf}{width=1\textwidth}{fig:PrecisionRN}{Variación de la precisión de la red neuronal según la proporción de imágenes reales y sintéticas.}



%-------------------------------------------------------------------
%\section*{\NotasBibliograficas}
%-------------------------------------------------------------------
%\TocNotasBibliograficas

%Citamos algo para que aparezca en la bibliografía\ldots
%\citep{ldesc2e}

%\medskip

%Y también ponemos el acrónimo \ac{CVS} para que no cruja.

%Ten en cuenta que si no quieres acrónimos (o no quieres que te falle la compilación en ``release'' mientras no tengas ninguno) basta con que no definas la constante \verb+\acronimosEnRelease+ (en \texttt{config.tex}).


%-------------------------------------------------------------------
%\section*{\ProximoCapitulo}
%-------------------------------------------------------------------
%\TocProximoCapitulo
