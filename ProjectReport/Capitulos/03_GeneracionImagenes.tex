%---------------------------------------------------------------------
%
%                          Capítulo 3
%
%---------------------------------------------------------------------

\chapter{Generación de imágenes}
\label{cap3}

\begin{resumen}
Para dotar a la aplicación de las capacidades de reconocimiento de imágenes es necesario pasar por un proceso previo de entrenamiento con cientos o miles de de ellas correctamente etiquetadas. La obtención de un conjunto de entrenamiento con las características requeridas es una tarea complicada. En este capítulo se describe una opción alternativa que permite generar esas imágenes de forma sintética a partir de modelos tridimensionales. 
\end{resumen}

%Para dotar a la aplicación d las capacidades de reconocimiento de imágenes es necesario pasar por un proceso previo de entrenamiento con cientos o miles de imágenes correctamente etiquetadas. La disponibilidad de un conjunto de entrenamiento es crítica, pero difícil. En este capítulo se describe una opción alternativa a través de la que esas imágenes son generadas de forma sintética.
%-------------------------------------------------------------------
\section{Introducción}
%-------------------------------------------------------------------
\label{cap3:sec:introduccion}

El objetivo del proyecto es reconocer e identificar distintos desechos y su material principal, e indicar cómo deben reciclarse adecuadamente. Para ello, se utilizan técnicas de visión artificial, lo que conlleva el entrenamiento de una red neuronal. 
Para la identificación hay que usar redes supervisadas. Eso significa que se requieren fotos correctamente etiquetadas, indicando lo que contienen, para que el sistema aprenda a reconocerlas. Ese entrenamiento es crítico para que todo funcione, y para ello se necesita un gran número de imágenes

Conseguir tantas imágenes, distintas y claras, supone un trabajo costoso y lento; sin tener en cuenta el almacenaje de estas. 
Contamos con varias opciones para conseguir las imágenes. Una de ellas, y quizá la más obvia, es descargarlas de la red. Esta opción dista de ser la ideal debido a lo tedioso que resulta este procedimiento; buscar, seleccionar y descargar una a una miles de imágenes que cuenten con una buena resolución, es un proceso muy lento.

Otra forma es realizar las fotografías personalmente. Esta cuenta con varios problemas, el primero es que se necesita tener acceso a los objetos, y cuando se necesita hacer miles de imágenes distintas el número de objetos a fotografía es muy alto. 

Una posible solución para este problema es acudir a establecimientos donde los vendan y llevar a cabo allí las fotos. Uno de los inconvenientes es que obtener las imágenes llevaría una cantidad de tiempo excesiva. Una alternativa puede ser la grabación de vídeos, utilizando los \textit{frames} como imágenes de entrenamiento. Esta opción es más dinámica frente a las fotografías, pero por otro lado todos los \textit{frames} obtenidos tendrían que ser revisados con atención. Esto es necesario por si aparecen otros objetos además del deseado, ya que se deberían separar para el correcto etiquetado; o, incluso, si ni siquiera aparece.

En ambas opciones se presentan varios inconvenientes. Los más importantes son, que al realizarse en un establecimiento, existe el riesgo de violar la protección de datos de los clientes y los trabajadores de alrededor, además de que no esté permitido realizar fotografías ni grabaciones dentro del comercio. Otro factor menos significativo es que al tratarse de una aplicación de reciclaje sería preferible que se trate de objetos ya usados, como una lata abierta, una botella vacía o un papel arrugado.

Los avances en aprendizaje automático durante los últimos años han permitido que se convierta en un campo más accesible para interesados con diferentes niveles de conocimiento. Esto ha provocado un gran aumento de materiales disponibles de manera libre.

Una posibilidad que se presenta es utilizar \textit{datasets} de libre uso como COCO (Common Objects in Context)~\cite{cocodataset2015}. Aunque este resulta no ser tampoco la opción perfecta, es un acercamiento a una posible solución. Estos \textit{datasets} tan amplios cuentan con tal diversidad de objetivos para identificar que llegan a tener decenas de miles de imágenes. Para poder utilizarlo en este proyecto sería necesario examinarlo entero y seleccionar y separar aquellas que se pudieran utilizar. Esto, como en los casos anteriores, es una tarea sumamente lenta sin ninguna garantía de obtener imágenes suficientes.

Igual que existen \textit{datasets} tan amplios, también los hay pequeños y enfocados a la detección de uno o pocos objetos. A pesar de la limitación presente al tener que encontrar \textit{datasets} específicos para cada objeto que se quiera identificar, resulta la opción más accesible entre las mencionadas. 


Otro ámbito en el que se ha conseguido una gran mejoría en las últimas décadas es en los gráficos de contenidos digitales audiovisuales, ya sean películas, videojuegos, videoclips, anuncios, etc. Esta mejora llega hasta tal punto que a veces puede resultar difícil distinguir entre realidad y CGI (\textit{Computer-generated imagery}). Teniendo en cuenta esto, las dificultades mencionadas en la obtención de imágenes y las oportunidades que ofrece la automatización, no se ha querido desaprovechar la posibilidad de enfocarlo desde un punto más cercano a este. 

Debido a que las cámaras virtuales y las reales son sensores diferentes entrenando solamente con datos sintéticos la red sufre un desplome en la precisión, así que el uso de las imágenes generadas únicamente tampoco es una opción aceptable. Además, puesto que está demostrado que el uso de datos reales y sintéticos, combinados, mejora el rendimiento de la red frente al uso de datos reales únicamente~\cite{synthia2016} se considera que la mejor opción es utilizar los \textit{datasets} de menor tamaño en conjunto con imágenes generadas.

%Una industria que destaca por el realismo de sus productos es la de los videojuegos


%\todo{mencionar aquí los proyectos que han usado videojuegos o motores o que los mencionan. Enlazarlo con lo de los gráficos de contenidos digitales audiovisuales diciendo que es buena idea..}

%Esta es la opción que se eligió finalmente para realizar las imágenes de entrenamiento de la red neuronal. De esta forma el objetivo final de este apartado consiste en llevar a cabo una aplicación en la que se vayan cargando modelos tridimensionales de objetos y residuos diferente. Se les harán numerosas capturas con posiciones y rotaciones distintas y finalmente estas imágenes generadas serán posteriormente utilizadas para el entrenamiento. Gracias a esta aplicación se contará con el número de capturas que se quieran de cada objeto y material, pudiendo aumentar esta cantidad siempre que se desee o necesite.




%-------------------------------------------------------------------
\section{Aplicación}
%-------------------------------------------------------------------
\label{cap3:sec:aplicacion}

La aplicación de generación de imágenes ha sido desarrollada utilizando el motor de videojuegos Unity. Sus características principales, de la aplicación, son la carga de modelos tridimensionales y la toma de capturas. La aplicación cuenta con un Game Manager que se encarga de la gestión del trascurso de la aplicación y de las variables principales (rutas de directorios, cantidad de imágenes a generar o su extensión). Toma este nombre ya que es el que se suele dar en los juegos al gestor global, y se ha mantenido por inercia de uso de Unity.

Al importar los modelos, es necesario crear un objeto reutilizable de Unity para trabajar con ellos de manera  sencilla, denominados \textit{prefabs}. Estos se separan y guardan en carpetas según el material al que pertenecen (metal, plástico, vidrio); lo que facilitará posteriormente el guardado de las capturas generadas.

Durante el tiempo de vida de la aplicación se accede a cada una de estas carpetas para cargar, uno a uno, los \textit{prefabs} contenidos en ellas.
En cada frame se toma una captura de la escena, en ella se presenta uno de los modelos frente al fondo.
El modelo es colocado con una posición y rotación aleatorios diferentes en cada captura. De esta forma, a partir de un mismo objeto se obtienen imágenes muy distintas, ya que son capturados desde diversos ángulos y distancias.
Para que la posición aleatoria generada no quede fuera del campo de visión de la cámara, se establecen unos valores máximos y mínimos para cada eje de coordenadas. Aún así, para prevenir posibles errores, antes de hacer la captura se comprueba que el modelo se encuentra dentro del campo de visión.

Para generar más diversidad en las imágenes cada una cuenta con un fondo generado de manera aleatoria. Este se compone de dos planos superpuestos, separados levemente para que no se combinen, a los que se les asigna una textura de manera aleatoria. Uno de ellos siempre está visible, mientras que el otro sólo se muestra a veces, configurado con posición y rotación aleatorias (figura~\ref{fig:fondos}).

\begin{figure}
	\centering
	\subfloat[Captura con un plano de fondo.]{
		{{\includegraphics[width=0.6\textwidth]{Imagenes/unfondo.png}}}
		\label{fig:fondos:unplano}}
	\qquad
	\subfloat[Captura con dos planos de fondo.]{
		{{\includegraphics[width=0.6\textwidth]{Imagenes/dosfondos.png}}}
		\label{fig:fondos:dosplanos}}
	\caption{Capturas de ejemplo de los fondos.}
    \label{fig:fondos}
\end{figure}

Una vez establecido el objeto y el fondo, se toma la captura. Estas, igual que los modelos, se guardan separadas en carpetas según el material. Esto significa que las capturas de un objeto de metal se guardan en una carpeta llamada metal. Como con los modelos, todas las carpetas de los distintos materiales se almacenan en una carpeta raíz.  

Puesto que las imágenes generadas van a ser utilizadas para el entrenamiento de una red neuronal supervisada, todas ellas deben de estar correctamente etiquetadas. Como van a entrenarse utilizando TensorFlow Lite, sobre lo que se hablará en el capítulo~\ref{cap4}), el etiquetado se realiza de manera muy sencilla. 

Debido a que el objetivo de la aplicación no es identificar todos los objetos presentes en una imagen, sino identificar únicamente el material del objeto principal, no es necesario tener diversos objetos en cada imagen y delimitar cada uno de ellos con su etiqueta correspondiente. En este caso, el etiquetado se hace por jerarquía de carpetas. Esto quiere decir que hay una carpeta raíz que contiene distintas subcarpetas, una para cada tipo de material identificable por la red; y dentro de cada una de estas carpetas se almacenan todas las imágenes correspondientes. Para el entrenamiento de la red simplemente hay que indicar la ruta de la carpeta raíz y espera que dentro de esta estén las carpetas que serán las etiquetas y en su interior las imágenes de entrenamiento.

Un último punto importante es la unicidad de las capturas. Para ello se guardan usando el nombre del modelo añadiéndole la hora, minutos, segundos y milisegundos del momento en que ha sido tomada. 
Las capturas pueden exportarse en tanto en formato PNG como JPG, mediante la configuración del Game Manager. PNG es un formato sin pérdida que permite imágenes sin fondo. Puesto que esa propiedad no es necesaria en este proyecto, y que los resultados son similares con ambos formatos, es preferible el uso de JPG debido a que los archivos ocupan menos espacio. Esto es importante ya que se necesita un gran número de imágenes para el entrenamiento de la red neuronal y así no se ocupa tanto espacio en la máquina.

%El objeto instanciado cambia de posición y rotación a unas aleatorias en cada frame, para que sean distintas en cada captura. Para la posición se establecen unos límites entre los que se genera un valor aleatorio para cada eje. Para el eje vertical, el eje Y, se establece un valor propio en negativo para el mínimo y positivo para el máximo. Para el eje de la Z, la profundidad, se establece de manera similar, en este caso no se utiliza el mismo valor en simetría respecto al eje de coordenadas, sino que se tiene un valor máximo establecido por parámetro y el mínimo estará posicionado a dos unidades respecto a la cámara. Por último, la posición en el eje X, en horizontal, se genera también de manera aleatoria, pero el rango en vez de ser a partir de un valor pasado por parámetro en este caso se utiliza el generado para la posición en el eje Z; puesto en positivo y negativo. 

%Se decidió generar la posición en el eje X de esta manera puesto que según cuál sea la distancia del modelo respecto a la cámara, el rango en el eje X en el que es visible es directamente proporcional. De esta manera no se limita la posición a unos valores cerrados, sino que permite la posibilidad de generar muchas más posiciones viables. En el eje Y no se decidió realizar de esta misma manera ya que se observó, tras hacer numerosas pruebas, que este rango estuviera relacionado con la posición Z traía más problemas que ventajas. En la mayoría de los casos el rango acababa siendo demasiado amplio y se decidió que no merecía la pena ralentizar la aplicación con una operación cuando al final no generaba buenos resultados.

%Investigando al respecto se vio que esta clase suele generar problemas habitualmente, aunque no hay nada respaldado por Unity. En vista que lo que proporcionaba Unity no iba a ser útil se buscaron otras optativas decidiendo, finalmente, crear una clase propia para la captura de imágenes siguiendo un tutorial bien explicado\footnote{\url{https://www.youtube.com/watch?v=lT-SRLKUe5k}}. Las capturas se irán guardando en la carpeta destino dentro de una subcarpeta nombrada como el material correspondiente.


%-------------------------------------------------------------------
\subsection{Modelos 3D}
%-------------------------------------------------------------------
\label{cap3:sec:modelos3d}


Como se ha comentado, para generar las imágenes sintéticas se parte de modelos tridimensionales. Todos los modelos seleccionados y utilizados en este proyecto son de libre uso, conseguidos desde varias páginas dedicadas a esto (CGTrader\footnote{\url{https://www.cgtrader.com/}}, Free3D\footnote{\url{https://free3d.com/es/}}, TurboSquid\footnote{\url{https://www.turbosquid.com/}}, 3DModelHeaven\footnote{\url{https://3dmodelhaven.com/}} y Unity Asset Store\footnote{\url{https://assetstore.unity.com/}}). El formato con el que se ha trabajado es FBX, es el más extendido debido a su interoperabilidad entre aplicaciones de creación de contenido digital. Permite que los recursos tridimensionales tengan la mínima pérdida de datos entre los distintos \textit{softwares} de edición 3D.

Existen varias dificultades principales con la obtención de modelos tridimensionales. En primer lugar está el realismo del modelo. Puesto que las capturas son para el entrenamiento de una red que se va a usar sobre objetos reales, es importante que los modelos utilizados sean lo más fieles posible al objeto real que representan. Los modelos tridimensionales de este tipo conllevan un gran esfuerzo y trabajo por parte de un profesional que los cree. Por lo tanto, es comprensible que sea complicado encontrar este tipo de modelos de manera libre y gratuita. De todas formas, aceptando el uso de modelos que distan un poco más de la realidad se obtienen buenos resultados igualmente.
El segundo inconveniente surge al realizar la importación de los modelos en Unity, donde salen a relucir fallos de los modelos en cuanto a geometría, texturas y materiales. Esto, en algunos casos, termina provocando la inutilidad de los modelos, lo cual provoca que sea necesario su sustitución. 

En los modelos en los que más problema se encuentra son todos aquellos que presentan transparencias, como el vidrio o algunos plásticos. Al ser modelos que no requieren texturas, se dificulta su importación en Unity, ya que el material tiene que configurarse desde cero. Esto provoca que la cantidad de modelos encontrados, útiles se vea mermada considerablemente. Dejando como único material con suficientes recursos el metal

\begin{figure}
	\centering
	\subfloat[Imagen de referencia del modelo.]{
		{{\includegraphics[width=0.5\textwidth]{Imagenes/botellaOnline.jpg}}}
		\label{fig:modelo:online}}
	\qquad
	\subfloat[Modelo al ser cargado.]{
		{{\includegraphics[width=0.5\textwidth]{Imagenes/botellaBlender.png}}}
		\label{fig:modelo:editor}}
	\caption{Imágenes de ejemplo de un modelo con transparencia online y en el editor.}
    \label{fig:modelo}
\end{figure}

Un último factor, es la extensión de los modelos. Aunque generalmente se encuentran en formato FBX, u OBJ en su defecto, hay muchos modelos que mantienen la extensión del \textit{software} utilizado para crearlos, y por lo tanto no pueden utilizarse fuera de este.


%-------------------------------------------------------------------
\subsection{\textit{Dataset}}
%-------------------------------------------------------------------
\label{cap3:sec:dataset}

Al final del proceso explicado durante el presente capítulo, se recogieron los siguientes conjuntos de imágenes:

De imágenes reales se obtuvieron varios datasets desde la página Kaggle. Se cuenta con tres materiales diferentes, plástico, vidrio y metal; de los cuales se cuenta con al rededor de mil imágenes para cada uno.

Para generar las imágenes sintéticas sólo se cuentan con suficientes modelos de objetos de metal. Así que las imágenes generadas a partir de ellos se mezclan con las reales de este material en el entrenamiento de la red. Aunque se sabe que esto mejora el rendimiento~\cite{synthia2016}, se desconoce en qué proporciones se obtiene el mejor resultado. Tanto las pruebas de rendimiento, como el entrenamiento de la red, se explican en el capítulo~\ref{cap4:sec:entrenamiento}.




%-------------------------------------------------------------------
%\section*{\NotasBibliograficas}
%-------------------------------------------------------------------
%\TocNotasBibliograficas


%-------------------------------------------------------------------
%\section*{\ProximoCapitulo}
%-------------------------------------------------------------------
%\TocProximoCapitulo
