%---------------------------------------------------------------------
%
%                          Capítulo 5
%
%---------------------------------------------------------------------

\chapter{Aplicación de identificación de objetos}
\label{cap5}
\begin{resumen}
En este capítulo se trata el desarrollo de la aplicación móvil y se estudia su funcionamiento. Esta aplicación tiene como objetivo ayudar a los usuarios a cómo reciclar los objetos a los que se apunte con la cámara. Para eso, tiene que ser capaz de reconocerlos basándose en modelos entrenados previamente. Además, se pone a prueba su funcionamiento y se comparan los resultados con los de distintos modelos. La aplicación es para dispositivos con sistema operativo Android y está llevada a cabo en Android~Studio con Java y la librería de TensorFlow~Lite para introducir la identificación. 

\end{resumen}

%-------------------------------------------------------------------
\section{Introducción}
%-------------------------------------------------------------------
\label{cap5:sec:introduccion}
Como se ha comentado, la aplicación de identificación de objetos para reciclaje es el objetivo principal de este trabajo de fin de grado. Se trata de una aplicación para Android que, utilizando la cámara del dispositivo en el que está instalada, identifica el material del objeto al que está enfocando el usuario. En la interfaz aparecen las tres opciones con más probabilidad, acompañadas del porcentaje de confianza respecto a esa opción. Independientemente de cuántas etiquetas de materiales distintas haya, siempre se muestran las tres con mayor confianza.
La aplicación está constantemente interpretando lo que recibe desde la cámara, así que las etiquetas y sus respectivos porcentajes se actualizan ininterrumpidamente sin necesidad de estar tomando fotografías para cada elemento.




%-------------------------------------------------------------------
\section{Identificación de objetos}
%-------------------------------------------------------------------
\label{cap5:sec:identificacion-objetos}
\todo{añadir: Para el correcto funcionamiento, en Andorid, de la aplicación con el modelo entrenado son necesarias dos librerías.
La primera es la librería de tareas de TensorFlow~Lite, que cuenta con un conjunto de bibliotecas específicas de tareas potentes y fáciles de usar por los desarrolladores para crear experiencias de aprendizaje automático. Esta funciona varias plataformas y es compatible con Java y C++.
La segunda librería necesaria es la de compatibilidad, esta facilita la integración de modelos en la aplicación. Proporciona API de alto nivel que ayuda a transformar los datos de entrada en el formato requerido por el modelo, además de interpretar su salida.}


Para realizar la identificación se ha incorporado TensorFlow~Lite y sus librerías al proyecto en Android Studio. Esto permite importar un intérprete, el cual carga el modelo y permite ejecutarlo ofreciéndole una serie de datos de entrada, finalmente, tras ejecutarlo, se muestran por pantalla los resultados obtenidos.
El trabajo se ha basado en las guías y recomendaciones que ofrece TensorFlow~Lite en sus ejemplos como ayuda para incorporar y utilizar el modelo de la red neuronal entrenado previamente (capítulo~\ref{cap4}).

Durante el flujo de la aplicación se hace uso de la cámara del dispositivo, leyendo \textit{frames} desde esta y los convirtiéndolos a imágenes. Estos son los datos que se utilizan como entrada para la identificación. Dichos datos se reciben en formato \textit{bitmap}, matrices donde cada casilla tiene asignado un color y que en conjunto forman una imagen. 
Ese \textit{bitmap} es convertido a \textit{byte~buffer}, denominado \textit{IMG data}, haciéndolo legible para la identificación. Además, utilizar \textit{byte~buffers} como entrada permite que la API de Java sea más rápida\footnote{\url{https://www.tensorflow.org/lite/performance/best_practices?authuser=1}}. Estos datos se cargan en el intérprete de TensorFlow~Lite y finalmente se obtienen los valores de resultado. Estos valores obtenidos son índices respecto a las etiquetas más la probabilidad de que esa imagen corresponda a dicha etiqueta. Este resultado se devuelve en un array, correspondiendo cada posición a cada una de las etiquetas disponibles. Finalmente, se seleccionan las tres etiquetas con mayor probabilidad y son mostradas por la interfaz.

La interfaz de la aplicación se divide en dos zonas. En la figura~\ref{fig:app} se muestra la aplicación en funcionamiento, donde pueden apreciarse las distintas características de esta. En la parte central se refleja todo aquello que se recibe a través la cámara del dispositivo y en la parte inferior se sitúa un panel en el que aparecen los resultados de la identificación. Estos se muestran en orden según de qué material se considera que se trata acompañado del contenedor en el que se debe desechar y el porcentaje de confianza de cada elemento.

Como se comentó en el capítulo~\ref{cap3}, al hablar de la obtención del \textit{dataset} utilizado para el entrenamiento, para el prototipado se ha trabajado con tres materiales (metal, vidrio y plástico). En la aplicación se muestran las tres posibilidades de identificación sobre las que se tiene más confianza, lo que significa que en este caso se presentan todas las opciones disponibles. Esto es independiente de la cantidad de materiales disponibles identificables. Es decir, si como trabajo futuro la cantidad de materiales se ampliara, y no se alterara esta funcionalidad, se continuarían mostrando las tres opciones con mayor probabilidad.

El panel de los resultados puede desplegarse hacia arriba, de esta forma se visualizan varias características como la resolución con la que se recogen las imágenes, la rotación del dispositivo y el tiempo de inferencia, que es el tiempo que tarda en detectar de qué objeto se trata.
Además de estas características, cuenta con dos opciones configurables por el usuario que permiten la mejora de la aplicación. La primera es la elección de cuántos hilos utiliza la aplicación, estos permiten acelerar la ejecución de los operadores. Sin embargo, el aumento de hilos utilizados hará que se necesiten más recursos y batería. La otra opción, es elegir entre utilizar la CPU o la GPU. La GPU es uno de los aceleradores que TensorFlow~Lite puede aprovechar. En dispositivos de gama media o alta la GPU es más rápida que la CPU, lo que reduce notablemente el tiempo de inferencia.

\begin{figure}
	\centering
	\subfloat[]
		{{\includegraphics[width=0.3\textwidth]{Imagenes/lata0}}}
	\qquad
	\subfloat[]
		{{\includegraphics[width=0.3\textwidth]{Imagenes/lata1000}}}

	\caption{Aplicación de identificación desarrollada}
    \label{fig:app}
\end{figure}








%-------------------------------------------------------------------
\section{Pruebas}
%-------------------------------------------------------------------
\label{cap5:sec:pruebas}
Con todo lo necesario desarrollado, se llevan a cabo varias pruebas para comprobar el funcionamiento de la aplicación de identificación. Tomando los resultados en el apartado~\ref{cap4:sec:resultados} se decidió que el mejor de los modelos obtenidos era para el que en las latas se habían utilizado un 70\% de imágenes generadas, con un 93\% de precisión. No obstante, se ha querido corroborar su correcto funcionamiento una vez introducido el modelo en la aplicación y utilizándolo sobre objetos nuevos del mundo real. Para ello se ha comparado el rendimiento de este modelo con el de otros tres de los entrenados. Se han seleccionado, como pueden verse resaltados en la figura~\ref{fig:modelosSeleccionados}, los dos modelos extremos, correspondiendo a los compuestos por completo por imágenes sintéticas y por imágenes reales, respectivamente; el modelo con el mejor resultado respecto a precisión y facilidad de obtención del \textit{dataset}, con el 70\% de imágenes generadas; y, por último, el simétrico del anterior, con el 30\% de imágenes sintéticas.


\figura{ModelosSeleccionados.pdf}{width=1\textwidth}{fig:modelosSeleccionados}{Variación de la precisión de la red neuronal con los modelos seleccionados destacados}

Para las comparaciones se han cogido diversos objetos y se ha estudiado qué etiqueta cuenta con el mayor valor de confianza para cada uno. También se tiene en cuenta aquellos casos en que las dos primeras etiquetas tienen porcentajes muy similares y por lo tanto van cambiando entre primera y segunda posición.
Puesto que la aplicación tan sólo utiliza uno de los modelos, se ha desarrollado una versión alterada para la realización de las pruebas. En esta segunda versión, denominada aplicación de \textit{test}, se incorporan los cuatro modelos simultáneamente y para cada objeto identificable se muestran los resultados de todos ellos. Al contrario que con la aplicación original, en esta no se va cambiando la posición del material con mayor confianza, sino que el orden se mantiene estable, generando una tabla en la que puede apreciarse la oscilación de los porcentajes con cada objeto detectado. En dicha tabla las filas indican los tres tipos de materiales. En cambio, cada columna corresponde a cada uno de los modelos, estos se identifican indicando los porcentajes de imágenes generadas y reales usadas en su entrenamiento. Estas se expresan primero con el porcentaje de imágenes reales, acompañado de la letra ``R'' en mayúscula, y después el porcentaje de imágenes generadas, seguido de la letra ``G'', ambos separados por un guión entre medias. Es decir, el modelo con todas las imágenes reales sería representado por ``100R - 0G''. De todas formas, para mayor claridad se ordenan de izquierda a derecha según disminuye la cantidad de imágenes reales.

Como se trata de casos de prueba, se prescinde de la información sobre en qué contenedor debe reciclarse cada objeto, dejando así más espacio para los datos de los modelos. Además, para mayor claridad durante el uso de la aplicación de \textit{test}, la columna correspondiente al modelo seleccionado para la aplicación final se le da cierto énfasis para distinguirla del resto.

El objeto de estudio principal han sido los objetos metálicos, ya que son aquellos que han sido entrenados con imágenes generadas. Sin embargo, también se ha probado sobre objetos de los otros materiales para comprobar el rendimiento respecto a estos.

En la figura~\ref{fig:latas} se pueden observar los resultados respecto a distintos objetos metálicos. Casi todos los modelos detectan con un nivel alto de confianza que en los tres casos se trata de objetos metálicos. A excepción del modelo entrenado por completo con imágenes sintéticas, el cual los identifica como vidrio. Un caso destacable es el de la lata de conservas con el modelo completamente formado por imágenes reales, el cual la identifica mayoritariamente como vidrio, pero no llega a superar el 50\%. Este resultado se explica con la ausencia de imágenes de objetos de este tipo que contaba dicho \textit{dataset}, lo cual significa que el entrenamiento con las imágenes sintéticas está aportando información útil, e incluso mejorando, la identificación.

\begin{figure}
	\centering
	\subfloat[Lata de conservas]
		{{\includegraphics[width=0.4\textwidth]{Imagenes/appLataConserva}}}
	\qquad
	\subfloat[Lata de bebida]
		{{\includegraphics[width=0.4\textwidth]{Imagenes/appLataBebida}}}
	\qquad
	\subfloat[Lata de encurtidos]
		{{\includegraphics[width=0.4\textwidth]{Imagenes/appLataEncurtido}}}

	\caption{Comparación con distintos objetos metálicos}
    \label{fig:latas}
\end{figure}

Una observación importante que se descubre, es la dificultad, en todas las opciones, para distinguir los objetos de vidrio respecto a los de plástico. Para tres objetos de vidrio diferentes todas los identifican como plástico con más de un 60\% de confianza. Esto no genera mucha preocupación puesto que como seres humanos a veces también es costoso diferenciar plástico translúcido de vidrio a simple vista.




Un problema importante que se ha observado es la variabilidad del resultado dependiendo de la iluminación. Como se puede observar en la figura~\ref{fig:botellas} para el mismo objeto y con el mismo modelo, en este caso el generado a partir de solamente imágenes reales, se obtienen resultados diferentes al cambiarlo de posición.

\begin{figure}
	\centering
	\subfloat[100\% imágenes reales]
		{{\includegraphics[width=0.3\textwidth]{Imagenes/BotellaLata}}}
	\qquad
	\subfloat[100\% imágenes reales]
		{{\includegraphics[width=0.3\textwidth]{Imagenes/BotellaPlastico}}}

	\caption{Comparación con una botella de agua.}
    \label{fig:botellas}
\end{figure}

\figura{latacacao.jpg}{width=0.3\textwidth}{fig:latacacao}{Resultados del modelo 70-30 con una lata de bálsamo labial.}
%Para comprobar el correcto funcionamiento de la aplicación con los resultados del entrenamiento de la red se han hecho pruebas con tres de los resultados. para ello se han seleccionado los dos modelos extremos (todas las imágenes sintéticas o reales, respectivamente) y dos modelos intermedios de 70\% de imágenes generadas o sintéticas, respectivamente; para comparar el funcionamiento de la aplicación en los distintos casos.


%Una vez desarrollada la aplicación de identificación de objetos se probaron cuatro de los modelos entrenados para corroborar el correcto o mal funcionamiento, respectivamente, de cada uno. Para ello se eligieron  los modelos extremos con todas las imágenes reales o todas las imágenes sintéticas; el modelo intermedio, dividido en mitad y mitad; y el modelo con mejor precisión, repartido en 70\% imágenes reales y 30\% generadas.


%-------------------------------------------------------------------
%\section*{\NotasBibliograficas}
%-------------------------------------------------------------------
%\TocNotasBibliograficas

%Citamos algo para que aparezca en la bibliografía\ldots
%\citep{ldesc2e}

%\medskip

%Y también ponemos el acrónimo \ac{CVS} para que no cruja.

%Ten en cuenta que si no quieres acrónimos (o no quieres que te falle la compilación en ``release'' mientras no tengas ninguno) basta con que no definas la constante \verb+\acronimosEnRelease+ (en \texttt{config.tex}).


%-------------------------------------------------------------------
%\section*{\ProximoCapitulo}
%-------------------------------------------------------------------
%\TocProximoCapitulo



% Variable local para emacs, para  que encuentre el fichero maestro de
% compilación y funcionen mejor algunas teclas rápidas de AucTeX
%%%
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../Tesis.tex"
%%% End:
