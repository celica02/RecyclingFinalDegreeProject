%---------------------------------------------------------------------
%
%                          Capítulo 4
%
%---------------------------------------------------------------------
\chapter{Red Neuronal}

\begin{resumen}


\end{resumen}



%-------------------------------------------------------------------
\section{TensorFlow y TensorFlow Lite}
%-------------------------------------------------------------------
\label{cap4:sec:tensorflow}

El equipo de Google Brain comenzó a investigar en 2011 el uso de redes neuronales a gran escala para utilizarlo en los productos de la empresa. Como resultado desarrollaron DistBelief, su primer sistema de entrenamiento e inferencia escalable. A partir de este, basándose en la experiencia y en un entendimiento más completo de las necesidades y requerimientos del sistema ideal, desarrollaron TensorFlow. Este utiliza modelos de flujo de datos y los mapea en una gran variedad de diferentes plataformas, desde dispositivos móviles, como máquinas sencillas de una o varias GPU, hasta sistemas a gran escala de cientos de máquinas especializadas con miles de GPUs. 
La API de TensorFlow y las implementaciones de referencia fueron lanzadas como un paquete de código abierto bajo una licencia de Apache 2.0 en noviembre de 2015; puede encontrarse todo en su página web\footnote{\url{https://www.tensorflow.org/}} \citep*{tensorflow2015-whitepaper}.


Como el nombre de TesorFlow indica, las operaciones son llevadas a cabo por redes neuronales en \textit{arrays} multidimensionales de datos, mejor conocidos como tensores.
Puesto que la estructura son grafos de flujo de datos, en estos, los nodos corresponden a las operaciones matemáticas como sumas, multiplicaciones, factorización y demás; en cambio, los bordes corresponden a los tensores. \citep*{Karim2018}.

Como se ha comentado, TensorFlow fue desarrollado para ser ejecutado en sistemas muy diversos, incluidos dispositivos móviles. Este fue llamado TensorFlow Mobile y permitió a los desarrolladores para móvil crear aplicaciones interactivas sin los retrasos que generaban los cálculos computacionales de aprendizaje automático. A pesar de las optimizaciones para mejorar la actuación de los modelos y que el mínimo \textit{hardware} requerido era bastante accesible; seguía existiendo cuello de botella en la velocidad de cálculo computacional por la baja latencia de los dispositivos móviles. Por ejemplo, un dispositivo móvil cuyo \textit{hardware} era capaz de ejecutar 10 GFLOPS\footnote{\textit{Giga floating point operations per second}, proveniente de FLOPS, operaciones de coma flotante por segundo. } estaba limitado a ejecutar un modelo de 5 GFLOPS  a 2 FPS\footnote{\textit{frames} por segundo, del inglés \textit{frames per second}.}, lo que provocaba que la aplicación no funcionara como era esperado \citep*{Alsing2018}.

TensorFlow Lite es la evolución de TensorFlow Mobile, algunas de las optimizaciones que incluye son el uso de \textit{frameworks} como la API de redes neuronales de Android y redes neuronales optimizadas para móvil como MobileNets \citep*{howard2017mobilenets} y SqueezeNet\citep*{iandola2016squeezenet}.
Permite ejecutar modelos de aprendizaje profundo en dispositivos móviles. Estos son entrenados en una computadora y posteriormente trasladados al dispositivo sin necesidad de utilizar un servidor. Utiliza MobileNet, la cual está diseñada y optimizada para imágenes en móviles, incluyendo detección y clasificación de objetos, detección de caras y reconocimiento de lugares.
Los modelos de TensorFlow Lite generados en el entrenamiento tienen como extensión de archivo \textit{.tflite} el cual tiene formato FlatBuffer.
Tensorflow Lite no se limita a los modelos que han sido directamente creados con esta extensión, sino que en la documentación de TensorFlow se encuentra un conversor que toma un modelo en otro formato y lo convierte a \textit{.tflite}\footnote{\url{https://www.tensorflow.org/lite/convert?hl=es-419}}.


%-------------------------------------------------------------------
\section{Entrenamiento}
%-------------------------------------------------------------------
\label{cap4:sec:entrenamiento}


La fase del entrenamiento del proyecto está basada en las recomendaciones para generar modelos de Tensoflow Lite, ya que el modelo que se necesita es el de este formato.
Para llevar a cabo el entrenamiento de la red neuronal se ha realizado un \textit{script} que se encargue de la lógica de esto. Este \textit{script} está basado en el ejemplo de Tensorflow Lite y puede usarse desde Google Collab, donde está disponible para entrenar los modelos que se desee\footnote{\url{https://colab.research.google.com/drive/1sqBewUnvdATOO-yblj55EBFb2sM24XHR?hl=es-419}}. 

Para este proyecto se prefirió tener disponible el \textit{script} en el dispositivo y así evitar tener que cargar tantas imágenes a la plataforma y, además, el descargar el modelo constantemente para utilizarlo.
Basándose en el que proporciona TensorFlow de ejemplo, el archivo resulta bastante similar pero con unos cambios personalizados. El ejemplo está planteado para cargar todas las imágenes y después separarlas en entrenamiento y \textit{test}, cada grupo tendrá el porcentaje de imágenes que se le establezca. Por ejemplo, por defecto se reparten en el 90\% de las imágenes para entrenamiento y el 10\% restante para validación.
Pero esta no es la funcionalidad requerida para este proyecto. Puesto que el entrenamiento se va a realizar a partir de imágenes generadas y posteriormente se va a utilizar sobre objetos reales, para comprobar la viabilidad del modelo es necesario que las imágenes de \textit{test} sean imágenes reales. Por lo tanto, en este caso, es necesario cargar dos carpetas diferentes para los dos grupos. Puesto que estas siguen teniendo que estar separadas por el material al que pertenecen(plástico, metal, vidrio, etc.) dentro de las dos carpetas para entrenamiento y \textit{test} tendrán que estar las imágenes separadas en subcarpetas según esta distribución. Para mayor claridad, la estructura de carpetas queda como se representa en la figura~\ref{fig:CarpetasSepar}. En esta separación se ha mantenido la proporción aproximada de 90\% de imágenes de entrenamiento y 10\% de validación para todos los materiales.

El entrenamiento se divide en varios \textit{epochs}, los \textit{epochs} son el número de veces que el algoritmo recorre todos los datos. Estos datos se dividen en \textit{batchs}, en cada uno de estos se recoge una pequeña parte de los datos, y cada \textit{batch} se recorre en una iteración. Cada \textit{epoch} tiene varias iteraciones, tantas como para recorrer al completo el conjunto de datos. Esto resulta en que en cada \textit{epoch} el número de iteraciones es el resultado de dividir la cantidad de datos entre el tamaño de \textit{batch}. Por ejemplo, si se tienen 2400 imágenes y el tamaño de \textit{batch} es 32, en cada \textit{epotch} habría 75 iteraciones.
Para estos valores se han mantenido los de por defecto de TensorFlow ya que se ha considerado que eran adecuados; siendo el número de \textit{epochs} 5 y el tamaño de \textit{batch} 32.

\figura{Separadas.png}{width=.5\textwidth}{fig:CarpetasSepar}{Organización de las carpetas con las imágenes separadas.}

Con todo lo mencionado se genera el modelo entrenado con extensión \textit{.tflite} además de un documento de texto plano con las etiquetas de los distintos materiales. Estos dos archivos son lo que se han de trasladar a la aplicación móvil para ser utilizados.

%-------------------------------------------------------------------
\section{Resultados}
%-------------------------------------------------------------------
\label{cap4:sec:resultados}

Con las imágenes sintéticas generadas y el \textit{script} de entrenamiento de la red neuronal programado, se dio pie a investigar con qué porcentaje de imágenes sintéticas se obtenía un entrenamiento óptimo de la red que permitiera el correcto funcionamiento de la aplicación posteriormente. 
Esta investigación se ha llevado a cabo con las opciones para el prototipado de la aplicación, debido a las dificultades mencionadas en la sección~\ref{cap3:ssec:modelos3d} con la obtención de modelos tridimensionales. Esto significa que se ha limitado el entrenamiento a utilizar imágenes reales para dos de los tres materiales seleccionados (vidrio y plástico) y con el restante (lata), y con el que más imágenes podían generarse, llevar a cabo las pruebas y comparación de precisión de la red mezclando imágenes reales y las propias generadas.
Todas las imágenes utilizadas se han obtenido de datasets abiertos al uso público de Kaggle\footnote{\url{https://www.kaggle.com/}} 

Para la comparación se ha comenzando con sólo imágenes reales y se han ido aumentando paulatinamente en un 10\% las imágenes generadas hasta contar con un \textit{dataset} de entrenamiento de sólo imágenes sintéticas.
La figura \todo{meter gráfica entrenamiento} corresponde al crecimiento de la precisión de la red durante el entrenamiento; se encuentra representado para los distintos porcentajes de imágenes generadas y reales. En todos los casos crece de manera similar sin haber grandes diferencias según la proporción de imágenes sintéticas. 
Por otro lado, en la figura \todo{meter gráfica de test} se observa cómo varía la precisión al probarse el modelo con los datos de \textit{test}. La figura \todo{figura de la precisión final} muestra el valor final de la precisión en los distintos casos. A partir de esta última figura pueden sacarse las conclusiones de con qué porcentaje se obtendrían los mejores resultados. 

Una vez desarrollada la aplicación de identificación de objetos se probaron cuatro de los modelos entrenados para corroborar el correcto o mal funcionamiento, respectivamente, de cada uno. Para ello se eligieron  los modelos extremos con todas las imágenes reales o todas las imágenes sintéticas; el modelo intermedio, dividido en mitad y mitad; y el modelo con mejor precisión, repartido en 70\% imágenes reales y 30\% generadas.

%-------------------------------------------------------------------
%\section*{\NotasBibliograficas}
%-------------------------------------------------------------------
%\TocNotasBibliograficas

%Citamos algo para que aparezca en la bibliografía\ldots
%\citep{ldesc2e}

%\medskip

%Y también ponemos el acrónimo \ac{CVS} para que no cruja.

%Ten en cuenta que si no quieres acrónimos (o no quieres que te falle la compilación en ``release'' mientras no tengas ninguno) basta con que no definas la constante \verb+\acronimosEnRelease+ (en \texttt{config.tex}).


%-------------------------------------------------------------------
%\section*{\ProximoCapitulo}
%-------------------------------------------------------------------
%\TocProximoCapitulo
